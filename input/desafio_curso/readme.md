Documenta√ß√£o:

1) Foram definidos como dados a serem estudados os campos:
Dimens√£o Cliente:
- customer
- customerkey

Dimens√£o Localidade:
- city
- country
- region_name

Dimens√£o Tempo:
- Ano
- Dia
- M√™s

Fato Vendas:
- sales_amount


2) Para os campos com espa√ßos, como por exemplo o "Sales Amount", foi trocado o espa√ßo por _
Ex: Sales Amount --> Sales_Amount

3) Para a ingest√£o dos dados .csv para o hive, foi utilizado a seguinte forma:
- Dividido em 2 bancos, sendo um Stage e um Gerenciavel
- Tabelas Stages( ou externas) ficam no banco Stage
- Tabelas Gerenciaveis (ou fato) ficam no banco Gerenciavel

4) Deve-se primeiro execular o script install.sh que encontra-se na pasta "scripts/pre_process"
Esse script ir√° criar as tabelas fato e externa

5) Ap√≥s, deve-se executar os scripts que encontram-se na pasta "scripts/pre_process/malha"
Esses scripts ir√£o fazer a transfer√™ncia do arquivo .csv do ambiente Unix para o HDFS e em seguida realizar o tombamento (transferencia dos dados da tabela externa para gerenciavel)

6) As vari√°veis utilizadas nos scripts encontram-se no arquivo "config.sh"

7) a pasta "hql" encontram-se os hqls que ser√£o invocados pelos scripts de cria√ß√£o das tabelas externas (pasta "create_stage"), tabelas gerenciaveis (pasta "create_managed") e o insert (pasta "insert_table")

8) a pasta "unix_to_hdfs" possui os scripts que fazem a transfer√™ncia dos arquivos do ambiente Unix para o HDFS. Esses scripts s√£o chamados automaticamente quando rodada os cripts da MALHA.

9) Ap√≥s os dados estarem no HIVE, √© poss√≠vel ent√£o rodar o arquivo Python que far√° o carregamento dos dados bem como o tratamento do mesmo, conforme regra, onde campos de n√∫meros inteiros caso n√£o possuam dado ou estejam como NULL ser√° colocado o 0, e campos strings caso n√£o possuam dados ou esteja como NULL ser√° colocado "NAO ENCONTRADO".
Tal regra foi aplicado a TODOS os campos das tabelas.

10) Ap√≥s, √© realizad a cria√ß√£o do DataFrame Stage, o qual conter√° os joins das tabelas e todos os campos presentes nelas.

11) √â realizado tamb√©m inser√ß√£o de campos de data (Dia, mes, Ano) no DataFrame para que se possa criar a dimens√£o Tempo.

12) Ap√≥s todo o tratamento, os dados s√£o exportas, em arquivos .csv para a pasta "gold".

13) Os arquivos s√£o lidos pelo PowerBI para cria√ß√£o de gr√°ficos.

Obs1: Infelizmente n√£o foram feitos testes para verificar se as somas apresentadas na agrega√ß√£o realmente batem.
Obs2: Como o objetivo n√£o √© a parte visual, coloquei s√≥ 2 gr√°ficos no PowerBI s√≥ para mostrar as vendas x ano e vendas x cidade.
Obs3: Mesmo aplicando a transforma√ß√£o para ler o . no campo de valor como na forma Americana, n√£o consegui acertar no gr√°fico (n√£o entendi porque n√£o atualizou)
























DESAFIO BIG DATA/MODELAGEM

üìå ESCOPO DO DESAFIO
Neste desafio ser√£o feitas as ingest√µes dos dados que est√£o na pasta /raw onde vamos ter alguns arquivos .csv de um banco relacional de vendas.

 - VENDAS.CSV
 - CLIENTES.CSV
 - ENDERECO.CSV
 - REGIAO.CSV
 - DIVISAO.CSV

Seu trabalho como engenheiro de dados/arquiteto de BI √© prover dados em uma pasta desafio_curso/gold em .csv para ser consumido por um relat√≥rio em PowerBI que dever√° ser constru√≠do dentro da pasta 'app' (j√° tem o template).

üìë ETAPAS
Etapa 1 - Enviar os arquivos para o HDFS
    - nesta etapa lembre de criar um shell script para fazer o trabalho repetitivo (n√£o √© obrigat√≥rio)

Etapa 2 - Criar o banco DEASFIO_CURSO e dentro tabelas no Hive usando o HQL e executando um script shell dentro do hive server na pasta scripts/pre_process.

    - DESAFIO_CURSO (nome do banco)
        - TBL_VENDAS
        - TBL_CLIENTES
        - TBL_ENDERECO
        - TBL_REGIAO
        - TBL_DIVISAO

Etapa 3 - Processar os dados no Spark Efetuando suas devidas transforma√ß√µes criando os arquivos com a modelagem de BI.
OBS. o desenvolvimento pode ser feito no jupyter porem no final o codigo deve estar no arquivo desafio_curso/scripts/process/process.py

Etapa 4 - Gravar as informa√ß√µes em tabelas dimensionais em formato cvs delimitado por ';'.

        - FT_VENDAS
        - DIM_CLIENTES
        - DIM_TEMPO
        - DIM_LOCALIDADE

Etapa 5 - Exportar os dados para a pasta desafio_curso/gold

Etapa 6 - Criar e editar o PowerBI com os dados que voc√™ trabalhou.

No PowerBI criar gr√°ficos de vendas.
Etapa 7 - Criar uma documenta√ß√£o com os testes e etapas do projeto.

REGRAS
Campos strings vazios dever√£o ser preenchidos com 'N√£o informado'.
Campos decimais ou inteiros nulos ou vazios, devers√£o ser preenchidos por 0.
Atentem-se a modelagem de dados da tabela FATO e Dimens√£o.
Na tabela FATO, pelo menos a m√©trica <b>valor de venda</b> √© um requisito obrigat√≥rio.
Nas dimens√µes dever√° conter valores √∫nicos, n√£o dever√° conter valores repetidos.
para a dimens√£o tempo considerar o campo da TBL_VENDAS <b>Invoice Date</b>

INSTRU√á√ïES
voc√™s deveram me entregar o projeto no github e por email (zip)
meu email: cgomesf@minsait.com
nome do email: DESAFIO MINSAIT BI/BIGDATA (Aluno)
dentro deste email o zip e o link para o github onde estar√° o projeto.
prazo limite: at√© <b>24/06/2023<b>
Apos esta data n√£o poderei considerar para a nota.
